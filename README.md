# Awsome-Vision-MLPs
Collecting some MLPs with Computer-Vision (CV) papers.  If you find some ignored papers, please open issues or pull requests.


## Papers

### Transformer original paper

- [Attention is All You Need](https://arxiv.org/abs/1706.03762) (NIPS 2017)

- [Attention is not all you need: pure attention loses rank doubly exponentially with depth](https://arxiv.org/abs/2103.03404) -2021.05.05

### Technical blog

- [Chinese Blog] 3W字长文带你轻松入门视觉transformer [[Link](https://zhuanlan.zhihu.com/p/308301901)]
- [Chinese Blog] Vision Transformer , Vision MLP超详细解读 (原理分析+代码解读) (目录) [[Link](https://zhuanlan.zhihu.com/p/348593638)]

### Survey
  - Transformers in Vision: A Survey [[paper](https://arxiv.org/abs/2101.01169)]   - 2021.02.22
  - A Survey on Visual Transformer [[paper](https://arxiv.org/abs/2012.12556)]   - 2020.1.30

### arXiv papers
- **[S2-MLP]** S2-MLP: Spatial-Shift MLP Architecture for Vision [[paper](https://arxiv.org/abs/2106.07477)] 
- **[Graph-MLP]** Graph-MLP: Node Classification without Message Passing in Graph [[paper](https://arxiv.org/abs/2106.04051)] 
- When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations [[paper](https://arxiv.org/abs/2106.01548)] 
- **[Container]** Container: Context Aggregation Network [[paper](https://arxiv.org/abs/2106.01401)] 
- Can Attention Enable MLPs To Catch Up With CNNs? [[paper](https://arxiv.org/abs/2105.15078)] 
- **[MixerGAN]** MixerGAN: An MLP-Based Architecture for Unpaired Image-to-Image Translation [[paper](https://arxiv.org/abs/2105.14110)] 
- **[SegFormer]** SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers [paper](https://arxiv.org/abs/2105.15203) [[code](https://github.com/NVlabs/SegFormer)]
- Transformer-Based Deep Image Matching for Generalizable Person Re-identification [[paper](https://arxiv.org/abs/2105.14432)]
- Less is More: Pay Less Attention in Vision Transformers [[paper](https://arxiv.org/abs/2105.14217)] 
- Can Attention Enable MLPs To Catch Up With CNNs? [[paper](https://arxiv.org/abs/2105.15078)]
- **[ResMLP]** ResMLP: Feedforward networks for image classification with data-efficient training [[paper](https://arxiv.org/abs/2105.03404)] 
- Pay Attention to MLPs [[paper](https://arxiv.org/abs/2105.08050)]
- Do You Even Need Attention? A Stack of Feed-Forward Layers Does SurprisinglyWell on ImageNet [[paper](https://arxiv.org/abs/2105.02723#:~:text=A%20Stack%20of%20Feed%2DForward%20Layers%20Does%20Surprisingly%20Well%20on%20ImageNet,-Luke%20Melas%2DKyriazi&text=The%20strong%20performance%20of%20vision,their%20multi%2Dhead%20attention%20layers.)]  [[code](https://github.com/lukemelas/do-you-even-need-attention?utm_source=catalyzex.com)] 
- **[RepMLP]** RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition [[paper](https://arxiv.org/abs/2105.01883)] [[code](https://github.com/DingXiaoH/RepMLP?utm_source=catalyzex.com)]
- Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks [[paper](https://arxiv.org/abs/2105.02358)] 
- **[MLP-Mixer]** MLP-Mixer: An all-MLP Architecture for Vision [[paper](https://arxiv.org/abs/2105.01601)] 
